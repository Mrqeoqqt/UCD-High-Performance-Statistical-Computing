{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = tf.constant(C, name = 'C')\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(labels, C, axis = 0)\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot = [[0. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## function test\n",
    "labels = np.array([1,0,1,1,0])\n",
    "one_hot = one_hot_matrix(labels, C = 2)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 20557\n",
      "number of test examples = 5140\n",
      "X_train shape: (116, 20557)\n",
      "Y_train shape: (2, 20557)\n",
      "X_test shape: (116, 5140)\n",
      "Y_test shape: (2, 5140)\n"
     ]
    }
   ],
   "source": [
    "# input file path\n",
    "filename = r'D:\\wkspacePY\\STA 141C\\data\\processed.csv'\n",
    "# load and split data\n",
    "df = pd.read_csv(filename, index_col='shot_id')\n",
    "# training set\n",
    "# X_train: training matrix\n",
    "# Y_train: true label of training matrix\n",
    "# test set\n",
    "# X_test: test matrix\n",
    "# Y_test: true label of test matrix\n",
    "Y = df['shot_made_flag']\n",
    "X = df.drop(['shot_made_flag'], axis=1)\n",
    "Y = Y.as_matrix()\n",
    "X = X.as_matrix()\n",
    "# max_X = np.array([np.linalg.norm(X[:, i]) for i in range(X.shape[1])]).reshape(1, X.shape[1])\n",
    "max_X = np.array([np.max(X[:, i]) for i in range(X.shape[1])]).reshape(1, X.shape[1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# Normalize\n",
    "X_train /= max_X\n",
    "X_test /= max_X\n",
    "# Transpose\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "Y_train = Y_train.T\n",
    "Y_test = Y_test.T\n",
    "# convert to one_hot\n",
    "Y_train = one_hot_matrix(Y_train, C = 2)\n",
    "Y_test = one_hot_matrix(Y_test, C = 2)\n",
    "# print info\n",
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape = [n_x,None])\n",
    "    Y = tf.placeholder(tf.float32, shape = [n_y,None])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder:0\", shape=(116, ?), dtype=float32)\n",
      "Y = Tensor(\"Placeholder_1:0\", shape=(2, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## function test\n",
    "tf.reset_default_graph()\n",
    "X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nn):\n",
    "    \"\"\"\n",
    "    layer = len(nn)-1\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [n1, n0]\n",
    "                        b1 : [n1, 1]\n",
    "                        W2 : [n2, n1]\n",
    "                        b2 : [n2, 1]\n",
    "                        ...\n",
    "                        W_layer : [n(layer), n(layer-1)]\n",
    "                        b_layer : [n(layer), 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, ...\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    for i in range(len(nn)-1):\n",
    "        parameters['W' + str(i+1)] = tf.get_variable('W'+str(i+1), [nn[i+1], nn[i]], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        parameters['b' + str(i+1)] = tf.get_variable('b' + str(i+1), [nn[i+1],1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': <tf.Variable 'W1:0' shape=(25, 116) dtype=float32_ref>, 'b1': <tf.Variable 'b1:0' shape=(25, 1) dtype=float32_ref>, 'W2': <tf.Variable 'W2:0' shape=(12, 25) dtype=float32_ref>, 'b2': <tf.Variable 'b2:0' shape=(12, 1) dtype=float32_ref>, 'W3': <tf.Variable 'W3:0' shape=(2, 12) dtype=float32_ref>, 'b3': <tf.Variable 'b3:0' shape=(2, 1) dtype=float32_ref>}\n"
     ]
    }
   ],
   "source": [
    "## function test\n",
    "tf.reset_default_graph()\n",
    "neural_num = [X_train.shape[0], 25, 12, 2]\n",
    "parameters = {}\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters(neural_num)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", ...\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns: \n",
    "    Z(len(parameters)/2) -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    length = int(len(parameters)/2)\n",
    "    Z = tf.add(tf.matmul(parameters['W1'], X),parameters['b1'])\n",
    "    A = tf.nn.relu(Z)  \n",
    "    for i in range(2, length):\n",
    "        Z = tf.add(tf.matmul(parameters['W'+str(i)], A),parameters['b'+str(i)])\n",
    "        A = tf.nn.relu(Z)  \n",
    "    Z = tf.add(tf.matmul(parameters['W'+str(length)], A),parameters['b'+str(length)])\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_3 = Tensor(\"Add_2:0\", shape=(2, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## function test\n",
    "tf.reset_default_graph()\n",
    "neural_num = [X_train.shape[0], 25, 12, 2]\n",
    "parameters = {}\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(116, 1)\n",
    "    parameters = initialize_parameters(neural_num)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    print(\"Z_3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z_end, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z_end -- output of forward propagation (output of the last LINEAR unit), of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z_end\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z_end)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-816983b1e3e6>:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## function test\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(116, 1)\n",
    "    parameters = initialize_parameters(neural_num)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discarded\n",
    "def random_mini_batches(X_train, Y_train, minibatch_size, seed):\n",
    "    tf.set_random_seed(seed)\n",
    "    start = 0\n",
    "    interval = minibatch_size\n",
    "    data = np.vstack((Y_train, X_train))\n",
    "    np.random.shuffle(data)\n",
    "    end = start + interval\n",
    "    train_data = []\n",
    "    for i in range(minibatch_size):\n",
    "        if start >= X_train.shape[1]:\n",
    "            break\n",
    "        if end < X_train.shape[1]:\n",
    "            y = data[0:Y_train.shape[0], start:end]\n",
    "            x = data[Y_train.shape[0]:, start:end]\n",
    "        else:\n",
    "            y = data[0:Y_train.shape[0], start:]\n",
    "            x = data[Y_train.shape[0]:, start:]\n",
    "        train_data.append((x,y))\n",
    "        start = end\n",
    "        end = start + interval\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 20557)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## function test\n",
    "tf.reset_default_graph()\n",
    "train_data = random_mini_batches(X_train, Y_train, X_train.shape[1], 2)\n",
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, nn, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a len(nn)-1 layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->...->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set\n",
    "    Y_train -- test set\n",
    "    X_test -- training set\n",
    "    Y_test -- test set\n",
    "    nn -- input layer + number of neurals in each layer\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    # seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x,n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(nn)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z_end = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z_end, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:  \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            _ , tmp_cost = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train})\n",
    "            epoch_cost = tmp_cost\n",
    "            \n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(Z_end), tf.argmax(Y))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.719818\n",
      "Cost after epoch 100: 0.682979\n",
      "Cost after epoch 200: 0.673816\n",
      "Cost after epoch 300: 0.667770\n",
      "Cost after epoch 400: 0.663387\n",
      "Cost after epoch 500: 0.659919\n",
      "Cost after epoch 600: 0.656995\n",
      "Cost after epoch 700: 0.654455\n",
      "Cost after epoch 800: 0.652204\n",
      "Cost after epoch 900: 0.650219\n"
     ]
    }
   ],
   "source": [
    "## training and predict\n",
    "tf.reset_default_graph()\n",
    "start_time = time.time()\n",
    "neural_num = [X_train.shape[0], 25, 12, 2]\n",
    "# cost function of mini_batch may not decrease at each iteration\n",
    "parameters = model(X_train, Y_train, X_test, Y_test, neural_num, num_epochs = 15000, learning_rate = 10**(-2))\n",
    "end_time = time.time()\n",
    "print(\"Neural network training time consumed: %lf secs\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
